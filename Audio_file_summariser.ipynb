{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNwFh2YhLdEUc5vAKvC7BHP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f7995dd9da5b4b9280ddaaf2423cd5ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c45c61bfe72417cae17db5a88f4b249","IPY_MODEL_41efdb6ba4d3494495ff932003f52481","IPY_MODEL_e3286a31a03d4f829ee3a33b0523156f"],"layout":"IPY_MODEL_6c33386e22f549f9b8b48cf65b75da7a"}},"1c45c61bfe72417cae17db5a88f4b249":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e4d109ebde74ab8b734270a1d36214d","placeholder":"​","style":"IPY_MODEL_497081c52331436787f26705e6a2d343","value":"100%"}},"41efdb6ba4d3494495ff932003f52481":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_528ebff524dc4f678c454718af016919","max":24,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d3eec4f0d22466598cdea27437fbd77","value":24}},"e3286a31a03d4f829ee3a33b0523156f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ff3cad2a00d41109949d550004b5c5e","placeholder":"​","style":"IPY_MODEL_861ea72fbbff4eb2a5a1f148f2ccf382","value":" 24/24 [01:55&lt;00:00,  4.59s/it]"}},"6c33386e22f549f9b8b48cf65b75da7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e4d109ebde74ab8b734270a1d36214d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"497081c52331436787f26705e6a2d343":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"528ebff524dc4f678c454718af016919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d3eec4f0d22466598cdea27437fbd77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ff3cad2a00d41109949d550004b5c5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"861ea72fbbff4eb2a5a1f148f2ccf382":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Audio File Summarizer\n","\n","## Acknowledgment\n","I would like to thank the following individuals and organisations that made this project possible.\n","* Groq for providing me free access to their API key and thereby allowing me to gain hands-on experience in making API calls without having to constantly worry about token limits.\n","\n","## Audio Credits\n","Audio content from [Polyglot speaking in 7 languages](https://www.youtube.com/watch?v=esaXXVD0PTc), licensed under Creative Commons Attribution License (CC BY). Used for non-commercial, educational purposes.\n","\n","## Abstract\n","Audio file summarizer produces a list of summaries from an audio file in the language of the user's choice. It is useful for extracting meeting minutes out of the recording of a business meeting. The program can handle audio files containing multiple languages. In a business meeting between two companies with one party speaking in Japanese, another in Hebrew, and the common language being English, the program transcribes the utterances in all three languages, and produces meeting minutes in the language specified by the user.\n","<br>\n","The program works in the following sequence:\n","1. Ask the user to upload an audio file to their My Drive on Google Drive.\n","2. Transcribe the video in all the languages heard in the audio file.\n","3. Extract the key points.\n","4. Summarise each point in a bullet list.\n","5. Translate the bullet list into the language of the user's choice.\n","\n","## Prerequisite\n","To run the program, you need to set the Groq API key in Google Colab's Secrets.\n","\n","## Points of Consideration\n","We summarize in the original languages first, then translate the summary since it is generally more efficient and makes good practical sense.\n","* Translation has higher token-level cost (especially for long texts) in both time and API usage.\n","* Summarization reduces content size, which speeds up and simplifies the translation task.\n","* Summarizing in the original languages also preserves contextual and cultural nuances, which often get muddled if you translate first.\n","\n","## Challenges\n","* It took a long time to find a suitable multilingual audio file with an appropriate license that allows me to use in this project. An hour-long audio recording of a business meeting involving Japanese, Hebrew and English produced the best result. The model did not detect all the languages successfully in a couple of YouTube videos, including the one shown in the result.\n","* The \"large\" sized Whisper model processes audio files the best. The \"medium\", \"small\" and \"base\" sized models all show difficulty at detecting non-English languages.\n","* Running Whisper without a GPU affects its performance greatly, not just in terms of speed but also in terms of language detection performance. It takes about 15 minutes to process an 8-minute long audio file on the CPU. It takes about half that time to process the same file, and manages to show a better language detection rate when run on the GPU. If your PC is not equipped with up-to-date GPU card that is compatible with the GPU-enabled version of PyTorch, it is preferable to run the program in Google Colab using T4. The .ipynb file is provided in the repo folder (no Streamlit section).  \n","* I had to experiment iteractively and adjust the chunk size in milliseconds until I found an optimal size for capturing the conversational segments of various length. If a converstaional exchange in one language is short and that exchange goes into a chunk, together with part of the subsequent conversational exchange in a different language, Whisper failed to identify the language of the shorter segment. It meant each chunk was small (30 seconds), the overlap felt rather large (10 seconds) and the number of chunks being produced ended up being high, but it succeeded in coping with the frequent switchings in languages.\n","* Streamlit seems to be incompatibile with certain versions of torch (e.g. 2.6.0 CPU version). Upgrading the torch installation did not help.\n","* Streamlit also seems to find it difficult to cope with seven states. Up to two states is fine, but if there are more (I have not tested its threshold when it stops working correctly), it cannot return to the beginning of the loop to process another audio file where the starting state is clearly labelled and specified. Claude also could not find a workaround, hence the missing Start Over button in the program. Streamlit can work correctly with three variables."],"metadata":{"id":"DxWlRWgJZ3vW"}},{"cell_type":"markdown","source":["## Setup\n","\n","* Set the text wrap format in the Julyter Notebook file for enhanced readability\n","* Install the packages\n","* Access the LLM API key\n","* Initialise the model"],"metadata":{"id":"H6LOzqHHdxeN"}},{"cell_type":"code","source":["# Activate text wrapping in the output cells\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"91pCUUP3Afoh","executionInfo":{"status":"ok","timestamp":1745757112851,"user_tz":-180,"elapsed":7,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install groq langchain langchain_groq -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wuMXJPFYAuUR","executionInfo":{"status":"ok","timestamp":1745757126611,"user_tz":-180,"elapsed":10628,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"8230ac03-3ff9-4cfa-a319-1c0e79ad31a7"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/127.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/openai/whisper.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mTLP-sKrAvaT","executionInfo":{"status":"ok","timestamp":1745757236134,"user_tz":-180,"elapsed":107868,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"3f7e0a63-0283-4cd2-cb89-39b269179c21"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-brpnhlu0\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-brpnhlu0\n","  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n","Collecting tiktoken (from openai-whisper==20240930)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803706 sha256=43a9b762f1c543a1491a2538ad918e82a18043728e3b46db8e090ed4164e4b96\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0fkkysdy/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"]}]},{"cell_type":"code","source":["# Required by Whisper\n","!apt-get update && apt-get install -y ffmpeg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"s-xifih1BI5p","executionInfo":{"status":"ok","timestamp":1745173659411,"user_tz":-180,"elapsed":13312,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"3bd74231-3c2e-4b5f-b4ee-cc752f167ced"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n","Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,695 kB]\n","Fetched 22.2 MB in 5s (4,200 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"]}]},{"cell_type":"code","source":["# For manipulating the audio file\n","!pip install -q pydub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"__9jLYweBcRh","executionInfo":{"status":"ok","timestamp":1745173663802,"user_tz":-180,"elapsed":2326,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"93891c11-7145-4fc5-a1ae-9b7330454270"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Import the packages\n","import os\n","import re\n","import torch\n","import whisper\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from pydub import AudioSegment\n","from google.colab import drive, userdata\n","from langchain.prompts import PromptTemplate\n","from langchain_groq import ChatGroq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"jio8U4TM-sLJ","executionInfo":{"status":"ok","timestamp":1745173671587,"user_tz":-180,"elapsed":6393,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"a488fce2-0591-4536-b0dd-0e04e301a1e6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Mount Google Drive\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"s5zNDUmh_yc6","executionInfo":{"status":"ok","timestamp":1745173742266,"user_tz":-180,"elapsed":21296,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"86401bfb-b0a7-4b4c-cec8-da2131701dc3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["# Access the Groq API key\n","os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n","\n","# Initialize the language model\n","llm = ChatGroq(model=\"llama-3.3-70b-versatile\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"SccUe7Rf_wmK","executionInfo":{"status":"ok","timestamp":1745173745897,"user_tz":-180,"elapsed":712,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"e1cbb819-5894-4621-e743-d6eacc6e4334"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Get the user input\n","\n","Ask the user to specify:\n","* the name of the audio file (must be uploaded to My Drive on Google Colab)\n","* the language to write the meeting minutes in"],"metadata":{"id":"mcbU46qYd1RF"}},{"cell_type":"code","source":["# Get the name of the audio file from the user\n","def get_filename():\n","    \"\"\"Ask the user to enter the name of the audio file to summarise.\"\"\"\n","    filename = input(\"\\nPlease enter the name of the audio file. Make sure the audio file is in My Drive on Google Drive.\\n\")\n","    audio_file = f\"/content/drive/MyDrive/Colab_Notebooks/{filename}\"\n","    print(f\"\\nThe audio file path is: {audio_file}\")\n","    return audio_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"UjlXbjxW_t3K","executionInfo":{"status":"ok","timestamp":1745173748975,"user_tz":-180,"elapsed":37,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"78cf02a2-e785-430c-a0bf-e49202c470aa"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Ask the user which language to summarise the audio file into\n","def get_target_language():\n","    \"\"\"Ask the user which language to summarize the audio file into.\"\"\"\n","    target_language = input(\"\\nIn which language would you like to receive the summary?\\n\")\n","    print(f\"\\nThe target language is: {target_language}\")\n","    return target_language"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"bsPJdkfG_t-J","executionInfo":{"status":"ok","timestamp":1745173750487,"user_tz":-180,"elapsed":21,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"188b008c-9e4d-4a33-e362-dfdda5ee8050"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":["## Make a transcript of the audio file\n","Extract the content of the audio file into text using Whisper in chunks. The audio file of a business meeting is about an hour long, and processing it in one go causes Whisper to crash with memory or RAM issues."],"metadata":{"id":"lsejxmz5ebAW"}},{"cell_type":"code","source":["# Make a transcript of an audio file in chunks of 5 minutes\n","# Introduce a 20-second overlap between two adjacent chunks to maintain context continuity\n","\n","import pydub\n","\n","def make_transcript_in_chunks(audio_file_path, chunk_duration_ms=30000, overlap_ms=10000):\n","    \"\"\"\n","    Transcribe an audio file by splitting it into overlapping chunks to avoid memory issues.\n","\n","    Args:\n","        audio_file_path: Path to the audio file\n","        chunk_duration_ms: Duration of each chunk in milliseconds (30 seconds)\n","        overlap_ms: Overlap between chunks in milliseconds (10 seconds)\n","\n","    Returns:\n","        Full transcript of the audio file\n","    \"\"\"\n","    print(f\"Loading audio file: {audio_file_path}\")\n","\n","    audio = AudioSegment.from_file(audio_file_path)\n","\n","    # Get audio duration in milliseconds\n","    audio_duration = len(audio)\n","    print(f\"Audio duration: {audio_duration/1000:.2f} seconds\")\n","\n","    # Create a temporary directory for chunks if it doesn't exist\n","    if not os.path.exists(\"temp_chunks\"):\n","        os.makedirs(\"temp_chunks\")\n","\n","    # Calculate positions for chunking (with overlap)\n","    chunk_positions = list(range(0, audio_duration, chunk_duration_ms - overlap_ms))\n","\n","    # Ensure last chunk doesn't exceed audio length\n","    if chunk_positions and chunk_positions[-1] + chunk_duration_ms > audio_duration:\n","        chunk_positions[-1] = max(0, audio_duration - chunk_duration_ms)\n","\n","    # Add the final chunk if needed\n","    if chunk_positions and chunk_positions[-1] + chunk_duration_ms < audio_duration:\n","        chunk_positions.append(audio_duration - chunk_duration_ms)\n","\n","    # Load Whisper model - start with medium to balance accuracy and memory\n","    print(\"Loading Whisper model...\")\n","\n","    # Try to use GPU if available, otherwise fall back to CPU\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Using device: {device}\")\n","\n","    # Use a medium model if on CPU to improve speed\n","    # The base and small models are inadequate and do not detect non-English languages\n","    model_size = \"large\" if device == \"cuda\" else \"medium\"\n","    print(f\"Using {model_size} model\")\n","\n","    model = whisper.load_model(model_size, device=device)\n","\n","    # Initialize an empty list to store all transcriptions\n","    all_transcripts = []\n","\n","    # Process each chunk\n","    print(f\"Processing {len(chunk_positions)} chunks...\")\n","    for i, start_pos in enumerate(tqdm(chunk_positions)):\n","        # Extract chunk\n","        end_pos = min(start_pos + chunk_duration_ms, audio_duration)\n","        chunk = audio[start_pos:end_pos]\n","\n","        # Save chunk temporarily\n","        chunk_path = f\"temp_chunks/chunk_{i}.wav\"\n","        chunk.export(chunk_path, format=\"wav\")\n","\n","        # Clear CUDA cache to prevent memory issues\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","        # Transcribe chunk\n","        result = model.transcribe(chunk_path, task=\"transcribe\")\n","        all_transcripts.append({\n","            \"start\": start_pos / 1000,  # Convert to seconds\n","            \"end\": end_pos / 1000,\n","            \"text\": result[\"text\"].strip()\n","        })\n","        print(f\"Chunk {i+1}/{len(chunk_positions)} transcribed\")\n","\n","        # Clean up temporary chunk file\n","        os.remove(chunk_path)\n","\n","    # Clean up the temporary directory\n","    os.rmdir(\"temp_chunks\")\n","\n","    # Merge transcripts with overlap handling\n","    merged_transcript = merge_transcripts_with_overlap_handling(all_transcripts)\n","\n","    return merged_transcript"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"XTe64ncl_pMt","executionInfo":{"status":"ok","timestamp":1745173753030,"user_tz":-180,"elapsed":35,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"6302e51f-ce24-475c-8155-f45273387e57"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Merge the transcript chunks and remove the overlaps\n","def merge_transcripts_with_overlap_handling(transcripts):\n","    \"\"\"\n","    Merge transcripts handling duplicated content in overlapping portions.\n","\n","    Args:\n","        transcripts: List of dictionaries with start, end, and text fields\n","\n","    Returns:\n","        Merged transcript\n","    \"\"\"\n","    if not transcripts:\n","        return \"\"\n","\n","    # Sort transcripts by start time\n","    transcripts = sorted(transcripts, key=lambda x: x[\"start\"])\n","\n","    # Initialize with the first transcript\n","    merged_text = transcripts[0][\"text\"]\n","\n","    for i in range(1, len(transcripts)):\n","        current_text = transcripts[i][\"text\"]\n","        previous_text = merged_text\n","\n","        # Find potential overlap in text\n","        overlap_found = False\n","        min_overlap_length = 5  # Minimum characters to consider as overlap\n","\n","        for overlap_length in range(min(len(previous_text), len(current_text)), min_overlap_length - 1, -1):\n","            if previous_text[-overlap_length:].lower() == current_text[:overlap_length].lower():\n","                merged_text = previous_text + current_text[overlap_length:]\n","                overlap_found = True\n","                break\n","\n","        # If no text overlap found, simply append with a space\n","        if not overlap_found:\n","            merged_text += \" \" + current_text\n","\n","    # Clean up any multiple spaces, newlines, etc.\n","    merged_text = re.sub(r'\\s+', ' ', merged_text).strip()\n","\n","    return merged_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"ntM8UqHi_l6h","executionInfo":{"status":"ok","timestamp":1745173755254,"user_tz":-180,"elapsed":9,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"792554fb-10fb-48d7-9686-e6dc39f7cdc8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Define prompts with instructions for multilingual content\n","summarisation_prompt = PromptTemplate(\n","    input_variables=[\"transcript\"],\n","    template=\"\"\"You are a professional transcription analyst skilled in multiple languages including Japanese, Hebrew, and English.\n","\n","Here is a transcript of a multilingual audio recording:\n","\n","{transcript}\n","\n","Please carefully analyze this transcript and:\n","\n","1. Identify all key points regardless of which language they appear in\n","2. Create a comprehensive yet concise bullet-point list of these key points\n","3. For each key point, maintain the original language it was spoken in\n","4. Ensure you don't miss important information in any language\n","5. Format your response as a clean, well-structured bullet list\n","\n","Your output should be a multilingual summary that captures the essential content from all languages present in the recording.\"\"\"\n",")\n","\n","translation_prompt = PromptTemplate(\n","    input_variables=[\"summary\", \"language\"],\n","    template=\"\"\"You are an expert multilingual translator with deep cultural knowledge.\n","\n","Here is a multilingual summary of key points from an audio recording:\n","\n","{summary}\n","\n","Please translate this entire summary into {language}, following these guidelines:\n","\n","1. Translate ALL points into {language} only\n","2. Preserve the original meaning, tone, and nuance of each point\n","3. Pay special attention to cultural references, idioms, and specialised terminology\n","4. Maintain the bullet-point format for clarity\n","5. Ensure your translation is natural and fluent in {language}\n","\n","Your goal is to create a translation that feels native to {language} speakers while accurately representing the original content.\"\"\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"pjy9a2Mt_hZK","executionInfo":{"status":"ok","timestamp":1745173756368,"user_tz":-180,"elapsed":24,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"c7d03cf3-45b1-44c5-9f29-67f7f1aa3db0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Make a transcript of the audio file\n","def transcribe_audio_file(audio_file):\n","    \"\"\"\n","    Transcribe audio file using chunking to avoid memory issues.\n","\n","    Args:\n","        audio_file: Path to the audio file\n","\n","    Returns:\n","        The transcript text\n","    \"\"\"\n","    print(\"Starting transcription process...\")\n","    transcript = make_transcript_in_chunks(audio_file)\n","\n","    # Save the transcript\n","    with open(\"/content/drive/MyDrive/Colab_Notebooks/transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(transcript)\n","\n","    print(\"\\nTranscript saved to transcript.txt\")\n","    print(\"\\nFirst 500 characters of transcript:\")\n","    print(transcript[:500] + \"...\")\n","\n","    return transcript"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"u_JMFDHc_b8i","executionInfo":{"status":"ok","timestamp":1745173757311,"user_tz":-180,"elapsed":30,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"124fc94c-891c-4946-d053-bb88711c2b97"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Summarise the content of the audio file\n","def summarise_and_translate(transcript, language):\n","    \"\"\"\n","    Take a transcript of an audio file, summarise the content into a list of key points,\n","    and translate it into a language of user's choice.\n","\n","    Args:\n","        transcript: The transcript of the audio file\n","        language: Target language for translation\n","\n","    Returns:\n","        A translated list of key points in the target language\n","    \"\"\"\n","    print(\"\\nGenerating multilingual summary of key points...\")\n","    summary = (summarisation_prompt | llm).invoke({\"transcript\": transcript}).content\n","\n","    print(\"\\nSummary generated. Now translating to\", language)\n","    translated_summary = (translation_prompt | llm).invoke({\"summary\": summary, \"language\": language}).content\n","\n","    # Save both versions\n","    with open(\"summary_original.txt\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(summary)\n","\n","    with open(f\"summary_{language}.txt\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(translated_summary)\n","\n","    return translated_summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"KNuoV-90_bJC","executionInfo":{"status":"ok","timestamp":1745173861242,"user_tz":-180,"elapsed":67,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"496ec592-0f89-444e-a22f-cbc8641ce46a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Main execution flow\n","def main():\n","    print(\"\\n===== Multilingual Audio File Summariser =====\")\n","\n","    # Get the user input\n","    audio_file = get_filename()\n","    target_language = get_target_language()\n","\n","    # Check available GPU memory\n","    if torch.cuda.is_available():\n","        print(f\"\\nGPU available: {torch.cuda.get_device_name(0)}\")\n","        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","    else:\n","        print(\"\\nNo GPU available, will use CPU (slower)\")\n","\n","    # Produce a transcript of the audio file\n","    print(\"\\n----- Starting Transcription -----\")\n","    transcript = transcribe_audio_file(audio_file)\n","\n","    # Summarise and translate\n","    print(\"\\n----- Starting Summarization and Translation -----\")\n","    final_summary = summarise_and_translate(transcript, target_language)\n","\n","    print(\"\\n===== Final Result =====\")\n","    print(final_summary)\n","\n","    return final_summary"],"metadata":{"id":"zB-uFD4V-tEx","executionInfo":{"status":"ok","timestamp":1745173862527,"user_tz":-180,"elapsed":21,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"9b551a3d-657e-4405-fd6a-4188e1c11e93"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Run the main function\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f7995dd9da5b4b9280ddaaf2423cd5ec","1c45c61bfe72417cae17db5a88f4b249","41efdb6ba4d3494495ff932003f52481","e3286a31a03d4f829ee3a33b0523156f","6c33386e22f549f9b8b48cf65b75da7a","5e4d109ebde74ab8b734270a1d36214d","497081c52331436787f26705e6a2d343","528ebff524dc4f678c454718af016919","7d3eec4f0d22466598cdea27437fbd77","7ff3cad2a00d41109949d550004b5c5e","861ea72fbbff4eb2a5a1f148f2ccf382"]},"id":"MWFZRb4O-tOZ","executionInfo":{"status":"ok","timestamp":1745174076944,"user_tz":-180,"elapsed":212737,"user":{"displayName":"Rena Bracha","userId":"11057790665540626640"}},"outputId":"b1fe07ce-51ab-4f91-8f60-db609e8ee593"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","===== Multilingual Audio File Summariser =====\n","\n","Please enter the name of the audio file. Make sure the audio file is in My Drive on Google Drive.\n","7_langs_mix.mp3\n","\n","The audio file path is: /content/drive/MyDrive/Colab_Notebooks/7_langs_mix.mp3\n","\n","In which language would you like to receive the summary?\n","English\n","\n","The target language is: English\n","\n","GPU available: Tesla T4\n","Available GPU memory: 15.83 GB\n","\n","----- Starting Transcription -----\n","Starting transcription process...\n","Loading audio file: /content/drive/MyDrive/Colab_Notebooks/7_langs_mix.mp3\n","Audio duration: 472.41 seconds\n","Loading Whisper model...\n","Using device: cuda\n","Using large model\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████| 2.88G/2.88G [00:47<00:00, 64.7MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing 24 chunks...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/24 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7995dd9da5b4b9280ddaaf2423cd5ec"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Chunk 1/24 transcribed\n","Chunk 2/24 transcribed\n","Chunk 3/24 transcribed\n","Chunk 4/24 transcribed\n","Chunk 5/24 transcribed\n","Chunk 6/24 transcribed\n","Chunk 7/24 transcribed\n","Chunk 8/24 transcribed\n","Chunk 9/24 transcribed\n","Chunk 10/24 transcribed\n","Chunk 11/24 transcribed\n","Chunk 12/24 transcribed\n","Chunk 13/24 transcribed\n","Chunk 14/24 transcribed\n","Chunk 15/24 transcribed\n","Chunk 16/24 transcribed\n","Chunk 17/24 transcribed\n","Chunk 18/24 transcribed\n","Chunk 19/24 transcribed\n","Chunk 20/24 transcribed\n","Chunk 21/24 transcribed\n","Chunk 22/24 transcribed\n","Chunk 23/24 transcribed\n","Chunk 24/24 transcribed\n","\n","Transcript saved to transcript.txt\n","\n","First 500 characters of transcript:\n","Bonjour mes amis, comment ça va? Je suis Marcos et je suis costarricense. Aujourd'hui je parlerai de mes langues. J'aime apprendre des langues étrangères. Je ne parle pas beaucoup de français, excusez-moi, mais je crois que le français est une langue très intéressante. Excusez-moi. Mais je crois que le français c'est une langue très intéressante. J'ai étudié le français à l'école mais je n'apprendais pas. Maintenant je comprends un petit peu si vous ne parlez pas très rapidement. Oui, je n'étudi...\n","\n","----- Starting Summarization and Translation -----\n","\n","Generating multilingual summary of key points...\n","\n","Summary generated. Now translating to English\n","\n","===== Final Result =====\n","* Hello my friends, how are you? - The speaker introduces themselves.\n","* I'm Marcos and I'm from Costa Rica - The speaker states their name and nationality.\n","* I love learning foreign languages - The speaker mentions their interest in learning foreign languages.\n","* I'm not studying French right now, but I would like to learn it one day - The speaker is not currently studying French but would like to learn it in the future.\n","* As for Italian, I understand it fairly well, but I don't speak it very fluently - The speaker understands some Italian but does not speak it well.\n","* I absolutely love Italian, I think it's a beautiful language, it's the most beautiful language in the world - The speaker loves Italian and thinks it's a beautiful language.\n","* I speak a little Russian - The speaker states they have some proficiency in Russian.\n","* I studied it and even helped with visa applications - The speaker mentions studying Russian and using it for practical purposes, such as visa applications.\n","* I don't have a lot of time to study - The speaker does not have much time to dedicate to learning.\n","* My stomach is working now - The speaker mentions their stomach is functioning properly (this is likely an expression similar to \"I'm feeling hungry\" or \"my stomach is growling\").\n","* I learned Portuguese a long time ago - The speaker learned Portuguese in the past.\n","* I used to work with Portuguese and spoke it a lot every day - The speaker used to work with Portuguese and spoke it frequently.\n","* I think I've forgotten a lot of vocabulary, many words - The speaker thinks they have forgotten a significant amount of Portuguese vocabulary.\n","* I studied German for many years, but I only know a few German people - The speaker studied German for a long time but only knows a few native German speakers.\n","* I think German is a very important language - The speaker believes German is a significant language.\n","* I speak Spanish, it's my native language and the language I speak - The speaker mentions Spanish is their first language and the language they use the most.\n","* I have a Costa Rican accent, which is from my country, and we have the particularity that we don't pronounce the letter R like they do in other countries - The speaker talks about their Costa Rican accent and the unique pronunciation of the letter R in their dialect.\n","* I still make a lot of mistakes in English, but I think that's okay - The speaker acknowledges making mistakes in English but thinks it's acceptable.\n","* Having an accent, that's okay, it just means you're a foreigner - The speaker views having an accent as a natural part of being a non-native speaker.\n","* Making mistakes, it just means you're making progress - The speaker sees making mistakes as a step in the learning process.\n","* Learning is very interesting and fun - The speaker finds the process of learning to be enjoyable and engaging.\n","* Pura vida - The speaker ends with a common Costa Rican phrase, which translates to \"pure life\" or \"simple life,\" and is often used to express a carefree or optimistic attitude.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XxH93GLRCWl6"},"execution_count":null,"outputs":[]}]}